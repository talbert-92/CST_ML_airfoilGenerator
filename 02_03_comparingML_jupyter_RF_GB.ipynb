{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe34ae6-9ae5-4c96-bb1b-0565871d60b0",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Section 1: Data Loading and Stratified K-Fold Cross-Validation Theory\n",
    "\n",
    "## What is Cross-Validation (CV)?\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model in a reliable way. \n",
    "Instead of simply splitting the data once into training and testing sets, CV splits the data multiple times \n",
    "and trains/tests the model on different subsets to estimate how well it generalizes to unseen data.\n",
    "\n",
    "## What is K-Fold Cross-Validation?\n",
    "\n",
    "- The data is split into K equally sized \"folds\" (subsets).\n",
    "- For each of the K iterations:\n",
    "  - Use K-1 folds to train the model.\n",
    "  - Use the remaining fold to test the model.\n",
    "- The model's performance is averaged over the K iterations to give a more robust estimate.\n",
    "\n",
    "## Why Stratified K-Fold?\n",
    "\n",
    "- When the dataset has imbalanced classes (e.g., more invalid than valid airfoils),\n",
    "  regular K-Fold may produce splits with very different class distributions.\n",
    "- Stratified K-Fold ensures that each fold preserves the percentage of samples for each class.\n",
    "- This is crucial for classification tasks to avoid biased or misleading performance estimates.\n",
    "\n",
    "## In This Notebook\n",
    "\n",
    "- We load the CST training dataset (valid and invalid labeled samples).\n",
    "- We prepare features (CST coefficients) and target (valid/invalid label).\n",
    "- We split off a test set for final evaluation.\n",
    "- We define a Stratified K-Fold cross-validator for fair and balanced model assessment.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712cf5b-b50e-41c1-8824-335426524841",
   "metadata": {},
   "source": [
    "## Overview of Models Being Compared\r\n",
    "\r\n",
    "### Random Forest (RF)\r\n",
    "\r\n",
    "- **Type**: Ensemble method based on bagging (Bootstrap Aggregating).\r\n",
    "- **How it works**:\r\n",
    "  - Builds multiple independent decision trees using random subsets of data and features.\r\n",
    "  - Each tree votes for the predicted class.\r\n",
    "  - Final prediction is made by majority voting.\r\n",
    "- **Strengths**:\r\n",
    "  - Robust to overfitting due to averaging many trees.\r\n",
    "  - Handles high-dimensional data well.\r\n",
    "  - Relatively fast to train and predict.\r\n",
    "- **Limitations**:\r\n",
    "  - May underfit if trees are too shallow.\r\n",
    "  - Treats each tree independently, so it may miss complex patterns.\r\n",
    "\r\n",
    "### Gradient Boosting (GB)\r\n",
    "\r\n",
    "- **Type**: Ensemble method based on boosting.\r\n",
    "- **How it works**:\r\n",
    "  - Builds decision trees sequentially.\r\n",
    "  - Each new tree focuses on correcting errors made by previous trees.\r\n",
    "  - Combines all trees weighted by learning rate.\r\n",
    "- **Strengths**:\r\n",
    "  - Often more accurate than Random Forests.\r\n",
    "  - Learns complex relationships by focusing on hard-to-predict samples.\r\n",
    "  - Flexible with tuning parameters to control overfitting.\r\n",
    "- **Limitations**:\r\n",
    "  - Training is slower due to sequential nature.\r\n",
    "  - More sensitive to hyperparameter tuning.\r\n",
    "  - Risk of overfitting if not properly regularized.\r\n",
    "\r\n",
    "### Why Compare These Two?\r\n",
    "\r\n",
    "- Both are powerful tree-based methods suitable for tabular data.\r\n",
    "- RF is a great baseline—easy to use and robust.\r\n",
    "- GB usually achieves better accuracy but requires more care.\r\n",
    "- Comparing them helps choose the best fit for your CST airfoilification metrics or the tuning process next!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162ca50-e023-498a-818e-89e1f277c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side Evaluation of Random Forest and Gradient Boosting for CST Airfoil Classification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cst_training_dataset.csv\")\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=[\"valid\", \"airfoil\"], errors='ignore')\n",
    "y = df[\"valid\"]\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}, Positive samples: {y.sum()}, Negative samples: {len(y) - y.sum()}\")\n",
    "\n",
    "# Split for hold-out test (optional)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define stratified k-fold cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Setup complete. Ready to start cross-validation on both models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897102e2-12f0-4e90-990f-3d43fb24b741",
   "metadata": {},
   "source": [
    "# Section 2: Cross-Validation & Performance Metrics\r\n",
    "\r\n",
    "## What is Cross-Validation?\r\n",
    "\r\n",
    "Cross-validation is a method to evaluate the generalization performance of a model by training and testing it on multiple subsets \r\n",
    "of data, reducing bias and variance in performance estimates.\r\n",
    "\r\n",
    "## Key Metrics to Evaluate Classification Performance:\r\n",
    "\r\n",
    "- **Accuracy**:  \r\n",
    "  The fraction of correct predictions out of all predictions.  \r\n",
    "  $$\r\n",
    "  \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{Total Samples}}\r\n",
    "  $$  \r\n",
    "  where TP = True Positives, TN = True Negatives.\r\n",
    "\r\n",
    "- **Precision** (Positive Predictive Value):  \r\n",
    "  Of all samples predicted as positive (valid), how many are actually positive?  \r\n",
    "  $$\r\n",
    "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\r\n",
    "  $$  \r\n",
    "  High precision means few false positives.\r\n",
    "\r\n",
    "- **Recall** (Sensitivity or True Positive Rate):  \r\n",
    "  Of all actual positive samples, how many did the model correctly identify?  \r\n",
    "  $$\r\n",
    "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\r\n",
    "  $$  \r\n",
    "  High recall means few false negatives.\r\n",
    "\r\n",
    "- **F1-Score**:  \r\n",
    "  Harmonic mean of precision and recall; balances both metrics.  \r\n",
    "  $$\r\n",
    "  F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\r\n",
    "  $$  \r\n",
    "  Useful when you want a balance between precision and recall.\r\n",
    "\r\n",
    "- **ROC AUC (Receiver Operating Characteristic Area Under Curve)**:  \r\n",
    "  Measures the ability of the model to distinguish between classes across all classification thresholds.  \r\n",
    "  A perfect classifier has ROC AUC = 1.0; a random classifier has 0.5.\r\n",
    "\r\n",
    "## What We Will Do:\r\n",
    "\r\n",
    "- Use Stratified K-Fold Cross-Validation to split training data into 5 folds, preserving class ratios.\r\n",
    "- Train both Random Forest and Gradient Boosting classifiers on each fold.\r\n",
    "- Compute above metrics for each fold.\r\n",
    "- Average the metrics to assess overall model quality.\r\n",
    "- Compare models side-by-side to understand strengths and weaknesses.\r\n",
    " strengths and weaknesses.\r\n",
    "\"\"\"side-by-side.\r\n",
    "\r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fae2231-0bc9-4d38-81b3-dcc8c90b54da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1...\n",
      "Fold 2...\n",
      "Fold 3...\n",
      "Fold 4...\n",
      "Fold 5...\n",
      "\n",
      "--- Cross-Validation Results ---\n",
      "\n",
      "Random Forest Performance:\n",
      " Accuracy : 0.9844 ± 0.0115\n",
      " Precision: 1.0000 ± 0.0000\n",
      " Recall   : 0.9063 ± 0.0697\n",
      " F1 Score : 0.9494 ± 0.0391\n",
      " ROC AUC  : 0.9968 ± 0.0056\n",
      "\n",
      "Gradient Boosting Performance:\n",
      " Accuracy : 0.9896 ± 0.0101\n",
      " Precision: 0.9895 ± 0.0211\n",
      " Recall   : 0.9479 ± 0.0577\n",
      " F1 Score : 0.9672 ± 0.0329\n",
      " ROC AUC  : 0.9952 ± 0.0078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define models with default parameters for baseline evaluation\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_model = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize lists to hold metrics for each fold\n",
    "rf_metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"roc_auc\": []}\n",
    "gb_metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": [], \"roc_auc\": []}\n",
    "\n",
    "# Perform stratified k-fold cross-validation on training data\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    print(f\"Fold {fold}...\")\n",
    "\n",
    "    # Split data into fold-specific training and validation sets\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # ---- Random Forest Training and Evaluation ----\n",
    "    rf_model.fit(X_tr, y_tr)\n",
    "    y_pred_rf = rf_model.predict(X_val)\n",
    "    y_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute metrics\n",
    "    rf_metrics[\"accuracy\"].append(accuracy_score(y_val, y_pred_rf))\n",
    "    rf_metrics[\"precision\"].append(precision_score(y_val, y_pred_rf))\n",
    "    rf_metrics[\"recall\"].append(recall_score(y_val, y_pred_rf))\n",
    "    rf_metrics[\"f1\"].append(f1_score(y_val, y_pred_rf))\n",
    "    rf_metrics[\"roc_auc\"].append(roc_auc_score(y_val, y_proba_rf))\n",
    "\n",
    "    # ---- Gradient Boosting Training and Evaluation ----\n",
    "    gb_model.fit(X_tr, y_tr)\n",
    "    y_pred_gb = gb_model.predict(X_val)\n",
    "    y_proba_gb = gb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute metrics\n",
    "    gb_metrics[\"accuracy\"].append(accuracy_score(y_val, y_pred_gb))\n",
    "    gb_metrics[\"precision\"].append(precision_score(y_val, y_pred_gb))\n",
    "    gb_metrics[\"recall\"].append(recall_score(y_val, y_pred_gb))\n",
    "    gb_metrics[\"f1\"].append(f1_score(y_val, y_pred_gb))\n",
    "    gb_metrics[\"roc_auc\"].append(roc_auc_score(y_val, y_proba_gb))\n",
    "\n",
    "print(\"\\n--- Cross-Validation Results ---\\n\")\n",
    "\n",
    "def print_avg_metrics(name, metrics_dict):\n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\" Accuracy : {np.mean(metrics_dict['accuracy']):.4f} ± {np.std(metrics_dict['accuracy']):.4f}\")\n",
    "    print(f\" Precision: {np.mean(metrics_dict['precision']):.4f} ± {np.std(metrics_dict['precision']):.4f}\")\n",
    "    print(f\" Recall   : {np.mean(metrics_dict['recall']):.4f} ± {np.std(metrics_dict['recall']):.4f}\")\n",
    "    print(f\" F1 Score : {np.mean(metrics_dict['f1']):.4f} ± {np.std(metrics_dict['f1']):.4f}\")\n",
    "    print(f\" ROC AUC  : {np.mean(metrics_dict['roc_auc']):.4f} ± {np.std(metrics_dict['roc_auc']):.4f}\")\n",
    "    print()\n",
    "\n",
    "print_avg_metrics(\"Random Forest\", rf_metrics)\n",
    "print_avg_metrics(\"Gradient Boosting\", gb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb4303-9048-464f-bae4-b99fe99f5331",
   "metadata": {},
   "source": [
    "| Metric    | Random Forest  | Gradient Boosting | Interpretation                                                           |\n",
    "| --------- | -------------- | ----------------- | ------------------------------------------------------------------------ |\n",
    "| Accuracy  | 98.44% ± 1.15% | 98.96% ± 1.01%    | Both models correctly classify most samples — extremely high accuracy.   |\n",
    "| Precision | 100% ± 0%      | 98.95% ± 2.11%    | RF never falsely labels invalid airfoils as valid, GB almost as perfect. |\n",
    "| Recall    | 90.63% ± 6.97% | 94.79% ± 5.77%    | GB is better at catching valid airfoils (fewer false negatives).         |\n",
    "| F1 Score  | 94.94% ± 3.91% | 96.72% ± 3.29%    | GB’s balance of precision and recall is slightly better.                 |\n",
    "| ROC AUC   | 99.68% ± 0.56% | 99.52% ± 0.78%    | Both models excellently separate valid vs invalid samples.               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9518483-4833-4541-a561-4fba85cf4bd3",
   "metadata": {},
   "source": [
    "## Interpreting Classification Metrics\r\n",
    "\r\n",
    "When evaluating classification models, here are some typical benchmarks to help you decide if the model is performing well:\r\n",
    "\r\n",
    "- **Accuracy**:  \r\n",
    "  - Closer to 1.0 (or 100%) is better.  \r\n",
    "  - Above ~0.9 is usually considered good, but can be misleading if classes are imbalanced.\r\n",
    "\r\n",
    "- **Precision**:  \r\n",
    "  - Measures how many predicted positives are actually correct.  \r\n",
    "  - Values above 0.9 indicate very few false positives.\r\n",
    "\r\n",
    "- **Recall**:  \r\n",
    "  - Measures how many actual positives the model correctly found.  \r\n",
    "  - Values above 0.9 indicate very few false negatives.\r\n",
    "\r\n",
    "- **F1 Score**:  \r\n",
    "  - The harmonic mean of precision and recall balances both.  \r\n",
    "  - Above 0.9 is generally strong; below 0.8 may need improvement.\r\n",
    "\r\n",
    "- **ROC AUC**:  \r\n",
    "  - Measures ability to distinguish classes across thresholds.  \r\n",
    "  - 1.0 is perfect; 0.5 means random guessing.  \r\n",
    "  - Values above 0.9 indicate excellent discrimination.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Note:**  \r\n",
    "No single metric is sufficient alone; always consider precision and recall together, especially in imbalanced datasets. For your CST airfoil classification:\r\n",
    "\r\n",
    "- High recall is important to not miss valid airfoils.\r\n",
    "- High precision ensures invalid airfoils are not mistakenly accepted.\r\n",
    "\r\n",
    "Aim for a balance based on your project needs.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3139b-1c33-4581-b954-d2a66bd53327",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model Tuning and Optimization\n",
    "\n",
    "## Why Do We Need to Tune Machine Learning Models?\n",
    "\n",
    "Machine learning models like Gradient Boosting (GB) have parameters, called **hyperparameters**, that control their complexity and learning behavior.  \n",
    "Choosing the right hyperparameters is crucial for:\n",
    "\n",
    "- Improving model accuracy on unseen data  \n",
    "- Preventing overfitting (model performs well on training data but poorly on new data)  \n",
    "- Reducing underfitting (model is too simple to capture patterns)\n",
    "\n",
    "## What Are Hyperparameters in Gradient Boosting?\n",
    "\n",
    "Some important GB hyperparameters include:\n",
    "\n",
    "- **max_iter**: Number of boosting iterations (trees). More trees can improve accuracy but increase training time and risk overfitting.  \n",
    "- **learning_rate**: Controls how much each tree contributes. Smaller values make training slower but often improve performance.  \n",
    "- **max_depth**: Maximum depth of individual trees. Deeper trees can model more complex patterns but risk overfitting.  \n",
    "- **min_samples_leaf**: Minimum samples required to form a leaf in a tree. Larger values make trees simpler and more robust.  \n",
    "- **max_leaf_nodes**: Maximum number of leaf nodes per tree. Limits complexity.  \n",
    "- **l2_regularization**: Adds penalty to avoid overfitting by discouraging overly complex trees.\n",
    "\n",
    "## How Do We Tune These Parameters?\n",
    "\n",
    "We use **Randomized Search Cross-Validation**:\n",
    "\n",
    "- Instead of trying every possible combination of hyperparameters (which can be huge), we randomly sample a fixed number of combinations (`n_iter=50` in the script).  \n",
    "- For each sampled combination, we perform **k-fold cross-validation** (here, 5 folds) to estimate model performance reliably.  \n",
    "- We score each model by a metric; here, the **F1-score** is used because it balances precision and recall.  \n",
    "- The hyperparameter combination with the best average F1-score across folds is selected.\n",
    "\n",
    "## What Happens After Tuning?\n",
    "\n",
    "- The best model with tuned hyperparameters is saved for future use.  \n",
    "- We evaluate this model on a hold-out test set to estimate its real-world performance.  \n",
    "- Metrics like accuracy, precision, recall, F1-score, and ROC AUC help us understand how well the model performs.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Tuning optimizes the GB model so it better distinguishes valid vs invalid CST airfoils, reducing errors and improving prediction reliability — this means you waste less time on bad airfoils and focus simulations on promising designs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0dfc05f-4031-4edc-9e8f-5ef88c0155ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best hyperparameters found:\n",
      "{'min_samples_leaf': 1, 'max_leaf_nodes': 100, 'max_iter': 500, 'max_depth': 3, 'learning_rate': 0.1, 'l2_regularization': 0.1}\n",
      "Tuned GB model saved as 'gb_cst_classifier_tuned.joblib'\n",
      "Performance on test set:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "ROC AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00        24\n",
      "\n",
      "    accuracy                           1.00       144\n",
      "   macro avg       1.00      1.00      1.00       144\n",
      "weighted avg       1.00      1.00      1.00       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cst_training_dataset.csv\")\n",
    "X = df.drop(columns=[\"valid\", \"airfoil\"], errors='ignore')\n",
    "y = df[\"valid\"]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base GB model\n",
    "gb_clf = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    \"max_iter\": [100, 200, 300, 400, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7, 10, None],\n",
    "    \"min_samples_leaf\": [1, 5, 10, 20],\n",
    "    \"max_leaf_nodes\": [15, 31, 50, 100, None],\n",
    "    \"l2_regularization\": [0.0, 0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Use F1-score as tuning metric (balance of precision & recall)\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # number of parameter settings sampled\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Run search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(random_search.best_estimator_, \"gb_cst_classifier_tuned.joblib\")\n",
    "print(\"Tuned GB model saved as 'gb_cst_classifier_tuned.joblib'\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_gb = random_search.best_estimator_\n",
    "y_pred = best_gb.predict(X_test)\n",
    "y_proba = best_gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "print(\"Performance on test set:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a4c79-40d7-4510-bd38-a38362fde9ba",
   "metadata": {},
   "source": [
    "# Random Forest Model Tuning and Optimization\n",
    "\n",
    "## Why Tune Random Forest?\n",
    "\n",
    "Random Forest (RF) models have hyperparameters that control their complexity and how they learn from data.  \n",
    "Tuning these hyperparameters helps improve model accuracy and generalization, preventing both overfitting and underfitting.\n",
    "\n",
    "## Important Hyperparameters Tuned\n",
    "\n",
    "- **`n_estimators`**: Number of decision trees in the forest.  \n",
    "  More trees usually improve performance but increase computation time.\n",
    "\n",
    "- **`max_depth`**: Maximum depth of each decision tree.  \n",
    "  Deeper trees can capture more complex patterns but may overfit.\n",
    "\n",
    "- **`min_samples_split`**: Minimum number of samples required to split an internal node.  \n",
    "  Higher values prevent splitting on small sample sets, reducing overfitting.\n",
    "\n",
    "- **`min_samples_leaf`**: Minimum number of samples required to be at a leaf node.  \n",
    "  Larger leaf sizes make the model more robust by smoothing predictions.\n",
    "\n",
    "- **`max_features`**: Number of features considered when looking for the best split.  \n",
    "  Controls randomness and diversity among trees, affecting bias-variance tradeoff.\n",
    "\n",
    "## How Tuning Was Performed\n",
    "\n",
    "We used **Randomized Search Cross-Validation**:\n",
    "\n",
    "- Instead of exhaustively testing all parameter combinations, this method samples a fixed number of random combinations from the defined ranges.  \n",
    "- For each sampled combination, a 5-fold stratified cross-validation was run to estimate performance.  \n",
    "- The evaluation metric used was **F1-score**, which balances precision and recall — crucial for imbalanced classification.  \n",
    "- The combination of hyperparameters with the highest average F1-score across folds was selected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a463ced0-0fdf-420a-9f6c-5f6dbc95a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best hyperparameters found:\n",
      "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 30}\n",
      "Tuned RF model saved as 'rf_cst_classifier_tuned.joblib'\n",
      "Performance on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       120\n",
      "           1       1.00      1.00      1.00        24\n",
      "\n",
      "    accuracy                           1.00       144\n",
      "   macro avg       1.00      1.00      1.00       144\n",
      "weighted avg       1.00      1.00      1.00       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cst_training_dataset.csv\")\n",
    "X = df.drop(columns=[\"valid\", \"airfoil\"], errors='ignore')\n",
    "y = df[\"valid\"]\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base RF model\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2'],  # replaced 'auto' with None\n",
    "}\n",
    "\n",
    "# Use F1 score as evaluation metric\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Save best RF model\n",
    "joblib.dump(random_search.best_estimator_, \"rf_cst_classifier_tuned.joblib\")\n",
    "print(\"Tuned RF model saved as 'rf_cst_classifier_tuned.joblib'\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Performance on test set:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f537b77-01cb-4866-8cc9-735db5162b2c",
   "metadata": {},
   "source": [
    "## What We Achieved\n",
    "\n",
    "- The tuned RF model achieved **perfect precision, recall, F1-score, and accuracy** on the test set, indicating excellent generalization.  \n",
    "- This tuned RF model is now saved and ready for deployment or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc28882-1a44-42c6-842c-92f21b4d61fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
